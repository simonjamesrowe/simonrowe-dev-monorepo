# Research: Profile Chat with MCP Tools

**Feature**: 009-profile-chat
**Date**: 2026-02-26

## R1: LLM Provider — Groq via Spring AI

**Decision**: Use Groq as the LLM provider, accessed through Spring AI's OpenAI-compatible client.

**Rationale**: Groq provides fast LPU-based inference with an OpenAI-compatible API. Spring AI 1.1.2 supports Groq by configuring the OpenAI starter with Groq's base URL (`https://api.groq.com/openai`). This avoids introducing a new SDK — we reuse `spring-ai-starter-model-openai`.

**Configuration**:
- Dependency: `spring-ai-starter-model-openai`
- Properties: `spring.ai.openai.base-url=https://api.groq.com/openai`, `spring.ai.openai.api-key=${GROQ_API_KEY}`, `spring.ai.openai.chat.model=llama-3.3-70b-versatile`
- Streaming: Supported via `chatModel.stream(prompt)` returning `Flux<ChatResponse>`
- Tool/Function calling: Supported on select models (llama3-70b, etc.)

**Alternatives considered**:
- OpenAI direct: Higher cost, no LPU acceleration
- Ollama local: Not suitable for production deployment
- Anthropic Claude: Higher cost per token for a portfolio chat feature

---

## R2: Chat Memory — Spring AI InMemoryChatMemory

**Decision**: Use `MessageWindowChatMemory` with `InMemoryChatMemoryRepository` for ephemeral, per-session conversation memory.

**Rationale**: The spec requires ephemeral sessions — no persistence. `InMemoryChatMemoryRepository` (backed by `ConcurrentHashMap`) is auto-configured by default in Spring AI. `MessageWindowChatMemory` maintains a sliding window of messages (default 20), dropping older messages while preserving system messages. The conversation ID maps directly to the UUID session key from the spec.

**Configuration**:
- Auto-configured by Spring AI (no explicit bean needed unless customising window size)
- Conversation ID = UUID session key generated by the frontend
- Window size: 20 messages (default, sufficient for ephemeral chat)
- Memory cleanup: Sessions evict naturally when the JVM reclaims the `ConcurrentHashMap` entries. For production, a TTL-based eviction strategy (e.g., Caffeine cache or scheduled cleanup) should be added to prevent memory leaks from abandoned sessions.

**Integration with ChatClient**:
```java
ChatClient chatClient = ChatClient.builder(chatModel)
    .defaultAdvisors(MessageChatMemoryAdvisor.builder(chatMemory).build())
    .build();

chatClient.prompt()
    .user(userMessage)
    .advisors(a -> a.param(ChatMemory.CONVERSATION_ID, sessionId))
    .call()
    .content();
```

**Alternatives considered**:
- `MongoChatMemoryRepository`: Unnecessary — spec explicitly requires ephemeral sessions
- `JdbcChatMemoryRepository`: Would require adding a relational DB — against constitution (MongoDB is primary)
- Custom session map: Spring AI already provides exactly what's needed

---

## R3: MCP Server — Spring AI MCP Server WebMVC

**Decision**: Use `spring-ai-starter-mcp-server-webmvc` with SSE transport and `@McpTool` annotations to expose profile data tools.

**Rationale**: The existing backend is a Spring Boot WebMVC application. The WebMVC MCP server starter integrates natively without requiring a reactive stack. SSE (Server-Sent Events) transport is the default and provides real-time tool invocation for both the built-in chat and external MCP clients. The `@McpTool` annotation allows declarative tool registration on existing service methods.

**Tools to expose**:
| Tool Name | Description | Input | Source Service |
|-----------|-------------|-------|----------------|
| `get_profile` | Get profile information | none | ProfileService |
| `search_blogs` | Search blog posts | query (string) | BlogSearchService / SearchService |
| `get_jobs` | Get employment history | none | JobService |
| `get_skills` | Get skills and skill groups | none | SkillGroupService |
| `search_site` | Search across all content | query (string) | SearchService |

**Configuration**:
```yaml
spring:
  ai:
    mcp:
      server:
        type: SYNC
        annotation-scanner:
          enabled: true
```

**Alternatives considered**:
- WebFlux MCP server: Would require adding reactive dependencies — unnecessary for this use case
- STDIO MCP server: Not suitable for web deployment
- Stateless HTTP: SSE is better for tool streaming scenarios
- Custom REST endpoints mimicking MCP: Would not be MCP-compatible for external agents

---

## R4: WebSocket for Chat Streaming

**Decision**: Use Spring WebSocket (STOMP over WebSocket) for real-time bidirectional chat communication with streaming responses.

**Rationale**: The user explicitly requested WebSocket connections. WebSocket provides true bidirectional communication — the client sends messages and receives streamed token-by-token responses. This is superior to SSE for chat because: (1) the client can send messages over the same connection without separate HTTP requests, (2) it enables real-time typing indicators, (3) it's the standard pattern for chat applications. Spring's `@EnableWebSocketMessageBroker` with STOMP provides structured message handling with topic/queue semantics.

**Architecture**:
- Client connects to WebSocket endpoint (e.g., `/ws/chat`)
- Client sends messages to `/app/chat/{sessionId}`
- Server streams responses to `/topic/chat/{sessionId}`
- STOMP provides message routing and subscription management
- Simple in-memory broker (no external message broker needed)

**Dependencies**:
- `spring-boot-starter-websocket`

**Alternatives considered**:
- SSE (Server-Sent Events): Unidirectional — client still needs HTTP POST for sending messages, less clean for chat
- Long polling: Higher latency, more server resources
- Raw WebSocket (no STOMP): Loses structured message routing, harder to manage topics

---

## R5: Rate Limiting

**Decision**: Use Bucket4j for in-memory, per-IP rate limiting on chat and MCP endpoints.

**Rationale**: The project has no existing rate limiting. Bucket4j provides a lightweight, in-memory token-bucket implementation that requires no external dependencies (no Redis, no database). It integrates with Spring MVC via a `HandlerInterceptor`. Per-IP limiting is appropriate since the feature is unauthenticated.

**Configuration**:
- Chat endpoint: 20 requests per minute per IP
- MCP tool endpoints: 60 requests per minute per IP
- Implementation: Spring `HandlerInterceptor` with `Bucket4j` local buckets stored in a `ConcurrentHashMap<String, Bucket>`
- Eviction: Caffeine cache with TTL to prevent memory leaks from unique IPs

**Dependencies**:
- `bucket4j-core` (no Spring Boot starter needed — just the core library)
- `com.github.ben-manes.caffeine:caffeine` (already transitively available via Spring Boot)

**Alternatives considered**:
- Spring Cloud Gateway rate limiting: Overkill — this is a monolithic app, not a gateway
- Resilience4j rate limiter: Less flexible for per-IP bucketing
- Custom counter-based limiter: Bucket4j is already well-tested and lightweight
- Guava RateLimiter: Single-rate only, no per-IP support built in

---

## R6: GraalVM Native Image Compatibility

**Decision**: Verify Spring AI and WebSocket compatibility with GraalVM native image; provide reflection hints if needed.

**Rationale**: The constitution mandates GraalVM native image via `bootBuildImage`. Spring AI 1.1.2 has reasonable native image support but may require additional GraalVM hints for reflection-heavy features (tool registration, JSON schema generation). WebSocket support in Spring Boot is native-image compatible with the standard Spring AOT processing.

**Risk mitigation**:
- Spring AI's `@McpTool` annotation processing may need `@RegisterReflectionForBinding` hints
- Bucket4j core library is lightweight and reflection-free
- Test native image build early in implementation

**Alternatives considered**: None — native image is a constitutional requirement, not optional.

---

## R7: Frontend Chat UI

**Decision**: Build a lightweight chat panel component that slides up from the bottom of the profile section, using native WebSocket API via STOMP.js client.

**Rationale**: The chat should feel lightweight and ephemeral. A bottom-anchored slide-up panel is a familiar chat pattern (similar to Intercom, Drift). On mobile, the panel expands to near-full-screen. The STOMP.js client library provides WebSocket STOMP protocol support for the browser.

**Frontend dependencies**:
- `@stomp/stompjs` — STOMP client for WebSocket communication

**Integration with search bar**: The `SiteSearch` component's `handleKeyDown` will be extended to handle Enter key — when Enter is pressed with a non-empty query, it emits a callback (e.g., `onChatStart(query)`) to the parent `ProfileBanner`, which opens the chat panel.

**Alternatives considered**:
- Full-page chat view: Too disruptive for a portfolio site
- Modal dialog: Less natural for ongoing conversation
- Native WebSocket without STOMP: Loses structured message routing
- Socket.io: STOMP.js is sufficient and aligns with Spring's STOMP support

---

## R8: Spring AI BOM / Version Management

**Decision**: Use Spring AI BOM 1.1.2 managed via `dependencyManagement` in the Gradle build, alongside the existing Spring Boot 3.5.x dependency management.

**Rationale**: Spring AI 1.1.2 publishes a BOM that manages all Spring AI module versions. Adding it to `dependencyManagement` ensures consistent versions across `spring-ai-starter-model-openai`, `spring-ai-starter-mcp-server-webmvc`, and other Spring AI modules.

**Gradle configuration**:
```kotlin
// In libs.versions.toml
springAi = "1.1.2"

// In build.gradle.kts
dependencyManagement {
    imports {
        mavenBom("org.springframework.ai:spring-ai-bom:${libs.versions.springAi.get()}")
    }
}
```

---

## R9: Chat Session Cleanup

**Decision**: Use a scheduled task to evict stale in-memory chat sessions after 30 minutes of inactivity.

**Rationale**: Since `InMemoryChatMemoryRepository` uses a `ConcurrentHashMap` internally, abandoned sessions will accumulate. A lightweight scheduled cleanup (every 5 minutes, evicting sessions older than 30 minutes) prevents memory leaks without adding external dependencies. The session activity timestamp is tracked separately.

**Alternatives considered**:
- Caffeine cache with TTL wrapping the memory repository: Requires customising the auto-configured bean
- No cleanup (rely on JVM restart): Unacceptable for long-running production deployments
- External session store (Redis): Against YAGNI — in-memory is sufficient for this scale
